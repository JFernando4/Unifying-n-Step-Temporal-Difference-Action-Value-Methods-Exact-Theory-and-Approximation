
convergent tree-backup
@article{ahmed_treebackup,
  author    = {Ahmed Touati and
               Pierre{-}Luc Bacon and
               Doina Precup and
               Pascal Vincent},
  title     = {Convergent Tree-Backup and Retrace with Function Approximation},
  journal   = {CoRR},
  volume    = {abs/1705.09322},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.09322},
  archivePrefix = {arXiv},
  eprint    = {1705.09322},
  timestamp = {Mon, 13 Aug 2018 16:46:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/TouatiBPV17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

tile coding
@article{Albus71atheory,
title = "A theory of cerebellar function",
journal = "Mathematical Biosciences",
volume = "10",
number = "1",
pages = "25 - 61",
year = "1971",
issn = "0025-5564",
doi = "https://doi.org/10.1016/0025-5564(71)90051-4",
url = "http://www.sciencedirect.com/science/article/pii/0025556471900514",
author = "James S. Albus"
}

Q(sigma, lambda)
@article{anna2017,
  author    = {Anna Harutyunyan and
              Peter Vrancx and
              Pierre{-}Luc Bacon and
              Doina Precup and
              Ann Now{\'{e}}},
  title     = {Learning with Options that Terminate Off-Policy},
  journal   = {CoRR},
  volume    = {abs/1711.03817},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.03817},
  archivePrefix = {arXiv},
  eprint    = {1711.03817},
  timestamp = {Mon, 13 Aug 2018 16:47:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-03817},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


tile coding
@book{Albus:1981:BBR:542806,
  title={Brains, behavior, and robotics},
  author={Albus, J.S.},
  isbn={9780070009752},
  lccn={81012310},
  series={Byte books},
  url={https://books.google.ca/books?id=mBJwAAAAIAAJ},
  year={1981},
  publisher={BYTE Books}
}

ALE
@Article{bellemare13arcade,
  author = {{Bellemare}, M.~G. and {Naddaf}, Y. and {Veness}, J. and {Bowling}, M.},
  title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},
  journal = {Journal of Artificial Intelligence Research},
  year = "2013",
  month = "jun",
  volume = "47",
  pages = "253--279",
}

count-based exploration
@article{bellemare2016,
  author    = {Marc G. Bellemare and
               Sriram Srinivasan and
               Georg Ostrovski and
               Tom Schaul and
               David Saxton and
               R{\'{e}}mi Munos},
  title     = {Unifying Count-Based Exploration and Intrinsic Motivation},
  journal   = {CoRR},
  volume    = {abs/1606.01868},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01868},
  archivePrefix = {arXiv},
  eprint    = {1606.01868},
  timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BellemareSOSSM16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@book{Bertsekas:1996:NP:560669,
 author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
 title = {Neuro-Dynamic Programming},
 year = {1996},
 isbn = {1886529108},
 edition = {1st},
 publisher = {Athena Scientific},
} 

@article{deasis2018,
  author    = {De Asis, Kristopher  and
              Richard Sutton},
  title     = {Per-decision Multi-step Temporal Difference Learning with Control 
              Variates},
  journal   = {CoRR},
  volume    = {abs/1807.01830},
  year      = {2018},
  url       = {https://arxiv.org/abs/1807.01830},
  archivePrefix = {arXiv},
  eprint    = {1807.01830},
  timestamp = {Thu, 5 Jul 2018 02:34:40 GMT}
}

@mastersthesis{deasis_msc_thesis,
  author    = {De Asis, Kristopher},
  title     = {A Unified View of Multi-step Temporal Difference Learning},
  year      = {2018},
  school    = {University of Alberta},
  note      =  {Submitted},
}

Xavier initialization
@INPROCEEDINGS{Glorot10understandingthe,
    author = {Xavier Glorot and Yoshua Bengio},
    title = {Understanding the difficulty of training deep feedforward neural networks},
    booktitle = {In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATSâ€™10). Society for Artificial Intelligence and Statistics},
    year = {2010}
}

DL book
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@INPROCEEDINGS{harm-hado-expected-sarsa,
author={H. van Seijen and H. van Hasselt and S. Whiteson and M. Wiering},
booktitle={2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning},
title={A theoretical and empirical analysis of Expected Sarsa},
year={2009},
volume={},
number={},
pages={177-184},
keywords={learning (artificial intelligence);stochastic processes;behavior policy;deterministic environment;expected Sarsa analysis;model-free reinforcement learning;on-policy temporal-difference method;stochasticity;zero variance;Artificial intelligence;Convergence;Dynamic programming;Intelligent systems;Optimal control;Probability distribution;Robot control;State estimation;State feedback;Supervised learning},
doi={10.1109/ADPRL.2009.4927542},
ISSN={2325-1824},
month={March},}

@article{Jaakkola:1994:CSI:1362288.1362296,
 author = {Jaakkola, Tommi and Jordan, Michael I. and Singh, Satinder P.},
 title = {On the Convergence of Stochastic Iterative Dynamic Programming Algorithms},
 journal = {Neural Comput.},
 issue_date = {November 1994},
 volume = {6},
 number = {6},
 month = nov,
 year = {1994},
 issn = {0899-7667},
 pages = {1185--1201},
 numpages = {17},
 url = {http://dx.doi.org/10.1162/neco.1994.6.6.1185},
 doi = {10.1162/neco.1994.6.6.1185},
 acmid = {1362296},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@inproceedings{Kearns:2000:BEB:648299.755183,
 author = {Kearns, Michael J. and Singh, Satinder P.},
 title = {Bias-Variance Error Bounds for Temporal Difference Updates},
 booktitle = {Proceedings of the Thirteenth Annual Conference on Computational Learning Theory},
 series = {COLT '00},
 year = {2000},
 isbn = {1-55860-703-X},
 pages = {142--147},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=648299.755183},
 acmid = {755183},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

Q(sigma, lambda) paper
@article{long2018,
  author    = {Long Yang and
               Minhao Shi and
               Qian Zheng and
               Wenjia Meng and
               Gang Pan},
  title     = {A Unified Approach for Multi-step Temporal-Difference Learning with
               Eligibility Traces in Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1802.03171},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.03171},
  archivePrefix = {arXiv},
  eprint    = {1802.03171},
  timestamp = {Mon, 13 Aug 2018 16:46:39 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-03171},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Lecun1985,
title = "Une procedure d'apprentissage pour reseau a seuil asymmetrique (A learning scheme for asymmetric threshold networks)",
author = "Yann Lecun",
year = "1985",
language = "English (US)",
pages = "599--604",
booktitle = "Proceedings of Cognitiva 85, Paris, France",
}

CNN
@inbook{Lecun1989,
title = "Generalization and network design strategies",
author = "Yann Lecun",
year = "1989",
language = "English (US)",
editor = "R. Pfeifer and Z. Schreter and F. Fogelman and L. Steels",
booktitle = "Connectionism in perspective",
publisher = "Elsevier",
}

Experience Replay
@phdthesis{Lin1992,
 author = {Lin, Long-Ji},
 title = {Reinforcement Learning for Robots Using Neural Networks},
 year = {1992},
 note = {UMI Order No. GAX93-22750},
 publisher = {Carnegie Mellon University},
 address = {Pittsburgh, PA, USA},
} 

Q(sigma lambda)
@article{markus_dumke,
  author    = {Markus Dumke},
  title     = {Double Q({\(\sigma\)}) and Q({\(\sigma\)}, {\(\lambda\)}): Unifying
               Reinforcement Learning Control Algorithms},
  journal   = {CoRR},
  volume    = {abs/1711.01569},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.01569},
  archivePrefix = {arXiv},
  eprint    = {1711.01569},
  timestamp = {Mon, 13 Aug 2018 16:48:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-01569},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{marlos_ale_rev,
  author    = {Marlos C. Machado and
               Marc G. Bellemare and
               Erik Talvitie and
               Joel Veness and
               Matthew J. Hausknecht and
               Michael Bowling},
  title     = {Revisiting the Arcade Learning Environment: Evaluation Protocols and
               Open Problems for General Agents},
  journal   = {CoRR},
  volume    = {abs/1709.06009},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.06009},
  archivePrefix = {arXiv},
  eprint    = {1709.06009},
  timestamp = {Thu, 05 Oct 2017 09:43:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-06009},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@inproceedings{precup2000,
	author={Precup, D. and Sutton, R. S. and Singh, S.},
	title={Eligibility traces for off-policy policy evaluation},
	year={2000},
	pages={759--766},
	editor={Morgan Kaufman},
	booktitle={Proceedings of the 17th International Conference on Machine Learning},
}

@article{qsigma-paper,
  author    = {Kristopher De Asis and
               J. Fernando Hernandez{-}Garcia and
               G. Zacharias Holland and
               Richard S. Sutton},
  title     = {Multi-step Reinforcement Learning: {A} Unifying Algorithm},
  journal   = {CoRR},
  volume    = {abs/ \allowbreak 1703.01327},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.01327},
  archivePrefix = {arXiv},
  eprint    = {1703.01327},
  timestamp = {Wed, 07 Jun 2017 14:40:30 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AsisHHS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{remi_retrace,
  author    = {R{\'{e}}mi Munos and
               Tom Stepleton and
               Anna Harutyunyan and
               Marc G. Bellemare},
  title     = {Safe and Efficient Off-Policy Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1606.02647},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.02647},
  archivePrefix = {arXiv},
  eprint    = {1606.02647},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MunosSHB16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Rumelhart:1986,
 author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
 chapter = {Learning Internal Representations by Error Propagation},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
 editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
 year = {1986},
 isbn = {0-262-68053-X},
 pages = {318--362},
 numpages = {45},
 url = {http://dl.acm.org/citation.cfm?id=104279.104293},
 acmid = {104293},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@phdthesis{rummery1995,
	author={Rummery, G. A.},
	year={1995},
	title={Problem Solving with Reinforcement Learning},
	school={Cambridge University},
}

ABQ(zeta)
@article{rupam2017,
  author    = {Ashique Rupam Mahmood and
               Huizhen Yu and
               Richard S. Sutton},
  title     = {Multi-step Off-policy Learning Without Importance Sampling Ratios},
  journal   = {CoRR},
  volume    = {abs/1702.03006},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.03006},
  archivePrefix = {arXiv},
  eprint    = {1702.03006},
  timestamp = {Mon, 13 Aug 2018 16:49:14 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MahmoodYS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shangtong2017,
  author    = {Shangtong Zhang and
               Richard S. Sutton},
  title     = {A Deeper Look at Experience Replay},
  journal   = {CoRR},
  volume    = {abs/1712.01275},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01275},
  archivePrefix = {arXiv},
  eprint    = {1712.01275},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01275},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Singh2000,
author="Singh, Satinder
and Jaakkola, Tommi
and Littman, Michael L.
and Szepesv{\'a}ri, Csaba",
title="Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms",
journal="Machine Learning",
year="2000",
month="Mar",
day="01",
volume="38",
number="3",
pages="287--308",
abstract="An important application of reinforcement learning (RL) is to finite-state control problems and one of the most difficult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot separate exploration from learning and therefore must confront the exploration problem directly. We prove convergence results for several related on-policy algorithms with both decaying exploration and persistent exploration. We also provide examples of exploration strategies that can be followed during learning that result in convergence to both optimal values and optimal policies.",
issn="1573-0565",
doi="10.1023/A:1007678930559",
url="https://doi.org/10.1023/A:1007678930559"
}

@article{sutton1988learning,
	Author = {Sutton, Richard S},
	Date-Added = {2017-04-23 20:24:18 +0000},
	Date-Modified = {2017-04-23 20:24:18 +0000},
	Journal = {Machine learning},
	Number = {1},
	Pages = {9--44},
	Publisher = {Springer},
	Title = {Learning to predict by the methods of temporal differences},
	Volume = {3},
	Year = {1988}
}

@inproceedings{sutton1996,
	author={Sutton, R S},
	year={1996},
	title={Generalization in reinforcement learning: Successful examples using   sparse coarse coding},
	booktitle={Advances in Neural Information Processing Systems 8},
	pages={1038--1044},
	publisher={MIT Press},
	editor={Touretzky, D. S. and Hasselmo, M. E.},
}

@book{Sutton:1998:IRL:551283,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@book{sutton2018,
	Author = {Sutton, R. S. and Barto, A. G.},
	Edition = {2nd},
	Note = {Manuscript in preparation},
	Title = {Reinforcement Learning: An Introduction},
	Year = {2018}
}

original GTD
@incollection{maei_gtd,
title = {A Convergent O(n) Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation},
author = {Sutton, Richard S and Hamid R. Maei and Csaba Szepesv\'{a}ri},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1609--1616},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3626-a-convergent-on-temporal-difference-algorithm-for-off-policy-learning-with-linear-function-approximation.pdf}
}

@misc{Tieleman2012,
  title={{Lecture 6e---RmsProp: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, T. and Hinton, G.},
  howpublished={COURSERA: Neural Networks for Machine Learning},
  year={2012}
}

@Article{Tsitsiklis1994,
author="Tsitsiklis, John N.",
title="Asynchronous stochastic approximation and Q-learning",
journal="Machine Learning",
year="1994",
month="Sep",
day="01",
volume="16",
number="3",
pages="185--202",
abstract="We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.",
issn="1573-0565",
doi="10.1007/BF00993306",
url="https://doi.org/10.1007/BF00993306"
}

@article{vanseijen2016,
  author    = {Harm van Seijen and
               Ashique Rupam Mahmood and
               Patrick M. Pilarski and
               Marlos C. Machado and
               Richard S. Sutton},
  title     = {True Online Temporal-Difference Learning},
  journal   = {CoRR},
  volume    = {abs/1512.04087},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.04087},
  archivePrefix = {arXiv},
  eprint    = {1512.04087},
  timestamp = {Wed, 07 Jun 2017 14:40:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SeijenMPMS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@phdthesis{watkins1989qlearn,
	author       = {Watkins, Christopher John Cornish Hellaby}, 
	title        = {Learning from Delayed Rewards},
	school       = {Cambridge University},
	year         = "1989"
}

@article{watkins1992,
	Author = {Watkins, Christopher John Cornish Hellaby and Dayan, Peter},
	Date-Added = {2017-04-23 20:31:05 +0000},
	Date-Modified = {2017-04-23 20:31:05 +0000},
	Journal = {Machine learning},
	Number = {3-4},
	Pages = {279--292},
	Publisher = {Springer},
	Title = "{Q}-learning",
	Volume = {8},
	Year = {1992}
}
